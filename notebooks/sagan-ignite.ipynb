{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# version 1\n",
        "# ---------------------------------\n",
        "# - SAGAN baseline\n",
        "# - num epochs = 450"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COMPUTE_LB = True"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os, time, glob, shutil\n",
        "starttime = time.time()\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET # for parsing XML\n",
        "from PIL import Image # to read images\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from argparse import Namespace\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from ignite.contrib.handlers import ProgressBar\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import ModelCheckpoint, Timer\n",
        "from ignite.metrics import RunningAverage"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    seed=123,\n",
        "    disable_cuda=False,\n",
        "    device=None,\n",
        "    debug=False,\n",
        "    num_workers=2,\n",
        "\n",
        "    ########## Ignite Stuff ###################\n",
        "    PRINT_FREQ = 600,\n",
        "    FAKE_IMG_FNAME = 'fake_sample_epoch_{:04d}.png',\n",
        "    REAL_IMG_FNAME = 'real_sample_epoch_{:04d}.png',\n",
        "    LOGS_FNAME = 'logs.tsv',\n",
        "    PLOT_FNAME = 'plot.svg',\n",
        "    SAMPLES_FNAME = 'samples.svg',\n",
        "    CKPT_PREFIX = 'networks',\n",
        "    output_dir = '../output_dir/',\n",
        "    alpha = 0.98, # smoothing constant for exponential moving average\n",
        "    \n",
        "    ######### Dataset Related #################\n",
        "    shuffle=True,\n",
        "    datapath=Path('/scratch/work/kumary1/dogs/all-dogs/'),\n",
        "    root_images=Path(\"/scratch/work/kumary1/dogs//all-dogs/\"),\n",
        "    root_annots=Path(\"/scratch/work/kumary1/dogs//Annotation/\"),\n",
        "    \n",
        "    ######### Training Params ######################\n",
        "    num_epochs=450,\n",
        "    lr=1e-4,\n",
        "    beta1 = 0.5, # for adam\n",
        "    batch_size=32,\n",
        "    weight_decay=0.001,\n",
        "    log_interval=100,\n",
        "    num_disc_update=3,\n",
        "\n",
        "    ######### Model Params #########################\n",
        "    image_size=64,\n",
        "    in_channels=3,\n",
        "    num_feature_maps_gen=64,\n",
        "    num_feature_maps_disc=64,\n",
        "    latent_dim=128,\n",
        "    input_dim=64*64,\n",
        "    hidden_size=400,    \n",
        "    dropout_p=0.2,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Random Seed: \", args.seed)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Eval Code\n",
        "#####################\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import gzip, pickle\n",
        "import tensorflow as tf\n",
        "from scipy import linalg\n",
        "import pathlib\n",
        "import urllib\n",
        "import warnings\n",
        "# from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "class KernelEvalException(Exception):\n",
        "    pass\n",
        "\n",
        "model_params = {\n",
        "    'Inception': {\n",
        "        'name': 'Inception', \n",
        "        'imsize': 64,\n",
        "        'output_layer': 'Pretrained_Net/pool_3:0', \n",
        "        'input_layer': 'Pretrained_Net/ExpandDims:0',\n",
        "        'output_shape': 2048,\n",
        "        'cosine_distance_eps': 0.1\n",
        "        }\n",
        "}\n",
        "\n",
        "def create_model_graph(pth):\n",
        "    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n",
        "    # Creates graph from saved graph_def.pb.\n",
        "    with tf.gfile.FastGFile( pth, 'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString( f.read())\n",
        "        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')\n",
        "\n",
        "def _get_model_layer(sess, model_name):\n",
        "    # layername = 'Pretrained_Net/final_layer/Mean:0'\n",
        "    layername = model_params[model_name]['output_layer']\n",
        "    layer = sess.graph.get_tensor_by_name(layername)\n",
        "    ops = layer.graph.get_operations()\n",
        "    for op_idx, op in enumerate(ops):\n",
        "        for o in op.outputs:\n",
        "            shape = o.get_shape()\n",
        "            if shape._dims != []:\n",
        "              shape = [s.value for s in shape]\n",
        "              new_shape = []\n",
        "              for j, s in enumerate(shape):\n",
        "                if s == 1 and j == 0:\n",
        "                  new_shape.append(None)\n",
        "                else:\n",
        "                  new_shape.append(s)\n",
        "              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n",
        "    return layer\n",
        "\n",
        "def get_activations(images, sess, model_name, batch_size=50, verbose=False):\n",
        "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
        "\n",
        "    Params:\n",
        "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
        "                     must lie between 0 and 256.\n",
        "    -- sess        : current session\n",
        "    -- batch_size  : the images numpy array is split into batches with batch size\n",
        "                     batch_size. A reasonable batch size depends on the disposable hardware.\n",
        "    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n",
        "                     batches is reported.\n",
        "    Returns:\n",
        "    -- A numpy array of dimension (num images, 2048) that contains the\n",
        "       activations of the given tensor when feeding inception with the query tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    inception_layer = _get_model_layer(sess, model_name)\n",
        "    n_images = images.shape[0]\n",
        "    if batch_size > n_images:\n",
        "        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n",
        "        batch_size = n_images\n",
        "    n_batches = n_images//batch_size + 1\n",
        "    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n",
        "    for i in tqdm(range(n_batches)):\n",
        "        if verbose:\n",
        "            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n",
        "        start = i*batch_size\n",
        "        if start+batch_size < n_images:\n",
        "            end = start+batch_size\n",
        "        else:\n",
        "            end = n_images\n",
        "                    \n",
        "        batch = images[start:end]\n",
        "        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n",
        "        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n",
        "    if verbose:\n",
        "        print(\" done\")\n",
        "    return pred_arr\n",
        "\n",
        "\n",
        "# def calculate_memorization_distance(features1, features2):\n",
        "#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n",
        "#     neigh.fit(features2) \n",
        "#     d, _ = neigh.kneighbors(features1, return_distance=True)\n",
        "#     print('d.shape=',d.shape)\n",
        "#     return np.mean(d)\n",
        "\n",
        "def normalize_rows(x: np.ndarray):\n",
        "    \"\"\"\n",
        "    function that normalizes each row of the matrix x to have unit length.\n",
        "\n",
        "    Args:\n",
        "     ``x``: A numpy matrix of shape (n, m)\n",
        "\n",
        "    Returns:\n",
        "     ``x``: The normalized (by row) numpy matrix.\n",
        "    \"\"\"\n",
        "    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "def cosine_distance(features1, features2):\n",
        "    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))\n",
        "    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))\n",
        "    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
        "    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
        "    norm_f1 = normalize_rows(features1_nozero)\n",
        "    norm_f2 = normalize_rows(features2_nozero)\n",
        "\n",
        "    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n",
        "    print('d.shape=',d.shape)\n",
        "    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n",
        "    mean_min_d = np.mean(np.min(d, axis=1))\n",
        "    print('distance=',mean_min_d)\n",
        "    return mean_min_d\n",
        "\n",
        "\n",
        "def distance_thresholding(d, eps):\n",
        "    if d < eps:\n",
        "        return d\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "            \n",
        "    Stable version by Dougal J. Sutherland.\n",
        "\n",
        "    Params:\n",
        "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
        "             inception net ( like returned by the function 'get_predictions')\n",
        "             for generated samples.\n",
        "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
        "               on an representive data set.\n",
        "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
        "               generated samples.\n",
        "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
        "               precalcualted on an representive data set.\n",
        "\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
        "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
        "        warnings.warn(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "    \n",
        "    # numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))\n",
        "\n",
        "    print('covmean.shape=',covmean.shape)\n",
        "    # tr_covmean = tf.linalg.trace(covmean)\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
        "    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n",
        "    \"\"\"Calculation of the statistics used by the FID.\n",
        "    Params:\n",
        "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
        "                     must lie between 0 and 255.\n",
        "    -- sess        : current session\n",
        "    -- batch_size  : the images numpy array is split into batches with batch size\n",
        "                     batch_size. A reasonable batch size depends on the available hardware.\n",
        "    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n",
        "                     batches is reported.\n",
        "    Returns:\n",
        "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
        "               the incption model.\n",
        "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
        "               the incption model.\n",
        "    \"\"\"\n",
        "    act = get_activations(images, sess, model_name, batch_size, verbose)\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma, act\n",
        "    \n",
        "def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n",
        "    path = pathlib.Path(path)\n",
        "    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
        "    imsize = model_params[model_name]['imsize']\n",
        "\n",
        "    # In production we don't resize input images. This is just for demo purpose. \n",
        "    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
        "    m, s, features = calculate_activation_statistics(x, sess, model_name)\n",
        "    del x #clean up memory\n",
        "    return m, s, features\n",
        "\n",
        "# check for image size\n",
        "def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n",
        "    im = Image.open(str(filename))\n",
        "    if is_checksize and im.size != (check_imsize,check_imsize):\n",
        "        raise KernelEvalException('The images are not of size '+str(check_imsize))\n",
        "    \n",
        "    if is_check_png and im.format != 'PNG':\n",
        "        raise KernelEvalException('Only PNG images should be submitted.')\n",
        "\n",
        "    if resize_to is None:\n",
        "        return im\n",
        "    else:\n",
        "        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n",
        "\n",
        "def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None, mm=[], ss=[], ff=[]):\n",
        "    ''' Calculates the KID of two paths. '''\n",
        "    tf.reset_default_graph()\n",
        "    create_model_graph(str(model_path))\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)\n",
        "        if len(mm) != 0:\n",
        "            m2 = mm\n",
        "            s2 = ss\n",
        "            features2 = ff\n",
        "        elif feature_path is None:\n",
        "            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n",
        "        else:\n",
        "            with np.load(feature_path) as f:\n",
        "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
        "\n",
        "        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n",
        "        print('starting calculating FID')\n",
        "        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
        "        print('done with FID, starting distance calculation')\n",
        "        distance = cosine_distance(features1, features2)        \n",
        "        return fid_value, distance, m2, s2, features2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### All utility functions\n",
        "def get_bbox(img_path):\n",
        "    \"image path as input and return list of bounding boxes around dogs (could be more than one per image)\"\n",
        "    annotation_basename = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    annotation_dirname = next(dirname for dirname in os.listdir(args.root_annots) if dirname.startswith(annotation_basename.split('_')[0]))\n",
        "    annotation_filename = os.path.join(args.root_annots, annotation_dirname, annotation_basename)\n",
        "    tree = ET.parse(annotation_filename)\n",
        "    root = tree.getroot()\n",
        "    objects = root.findall('object')\n",
        "    bboxes = []\n",
        "    for o in objects:\n",
        "        bndbox = o.find('bndbox')\n",
        "        xmin = int(bndbox.find('xmin').text)\n",
        "        ymin = int(bndbox.find('ymin').text)\n",
        "        xmax = int(bndbox.find('xmax').text)\n",
        "        ymax = int(bndbox.find('ymax').text)\n",
        "        bboxes.append((xmin, ymin, xmax, ymax))\n",
        "    return bboxes\n",
        "\n",
        "def doggo_loader(img_path):\n",
        "    img = torchvision.datasets.folder.default_loader(img_path) # default loader\n",
        "    bbox = get_bbox(img_path)[-1]\n",
        "    return img.crop(bbox)\n",
        "\n",
        "def clear_output_dir():\n",
        "    try:\n",
        "        shutil.rmtree(args.output_dir)\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "def check_gen_samples(dataloader, img_list):\n",
        "    \"Plot tile of real and generated images\"\n",
        "    \n",
        "    real_batch = next(iter(dataloader))\n",
        "    # Plot the real images\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Real Images\")\n",
        "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "    # Plot the fake images from the last epoch\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Fake Images\")\n",
        "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "    plt.show()\n",
        "    \n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DogDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform1, transform2=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_names = os.listdir(img_dir)\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "        \n",
        "        self.imgs = []\n",
        "        for img_name in tqdm(self.img_names):\n",
        "            img_path = os.path.join(img_dir, img_name)\n",
        "            img = Image.open(img_path)\n",
        "            bboxes = get_bbox(img_path)\n",
        "            \n",
        "            for bbox in bboxes:\n",
        "                img_crop = img.crop(bbox)\n",
        "                self.imgs.append(self.transform1(img_crop))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.imgs[index]\n",
        "        \n",
        "        if self.transform2 is not None:\n",
        "            img = self.transform2(img)\n",
        "        \n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "def get_transforms():\n",
        "    \n",
        "    # this normalizes pixel values between [-1,1]\n",
        "    # https://www.kaggle.com/jesucristo/gan-introduction565419\n",
        "    # GANHACK #1\n",
        "    normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    random_transforms = [transforms.ColorJitter(), \n",
        "                         transforms.RandomRotation(degrees=3)]\n",
        "    random_cropper = [torchvision.transforms.CenterCrop(args.image_size), torchvision.transforms.RandomCrop(args.image_size)]\n",
        "\n",
        "\n",
        "    # First preprocessing of data\n",
        "    transform1 = transforms.Compose([\n",
        "        transforms.Resize(args.image_size),\n",
        "        transforms.CenterCrop(args.image_size),\n",
        "    ])\n",
        "\n",
        "    transform2 = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomApply(random_transforms, p=0.4),\n",
        "        transforms.ToTensor(),\n",
        "        normalize]) \n",
        "    \n",
        "    return transform1, transform2\n",
        "\n",
        "transform1, transform2 = get_transforms()\n",
        "train_data = DogDataset(img_dir=args.root_images,\n",
        "                        transform1=transform1,\n",
        "                        transform2=transform2)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(train_data, \n",
        "                                         shuffle=True,\n",
        "                                         batch_size=args.batch_size, \n",
        "                                         num_workers=args.num_workers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "    \n",
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "        \n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N) \n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "        \n",
        "        out = self.gamma*out + x\n",
        "        return out, attention"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        repeat_num = int(np.log2(self.imsize)) - 3\n",
        "        mult = 2 ** repeat_num # 8\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer1.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = conv_dim * mult\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer2.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer3.append(nn.ReLU())\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "            layer4.append(nn.ReLU())\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn( 128, 'relu')\n",
        "        self.attn2 = Self_Attn( 64,  'relu')\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        out=self.l1(z)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out#, p1, p2\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=64, image_size=64, conv_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n",
        "        layer1.append(nn.LeakyReLU(0.1))\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer2.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer3.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "            layer4.append(nn.LeakyReLU(0.1))\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = curr_dim*2\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.Conv2d(curr_dim, 1, 4))\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn(256, 'relu')\n",
        "        self.attn2 = Self_Attn(512, 'relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out.squeeze()#, p1, p2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netG = Generator(args.batch_size, args.image_size, args.latent_dim, conv_dim=64).to(device)\n",
        "netD = Discriminator(args.batch_size, args.image_size, conv_dim=64).to(device)\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, args.latent_dim, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=args.lr/2.0, betas=(args.beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizerD, factor=0.5, patience=2, verbose=True)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizerD, gamma=0.5, step_size=8)\n",
        "\n",
        "def step(engine, batch):\n",
        "    real_cpu_batch = batch.to(device)\n",
        "    args.batch_size = real_cpu_batch.size(0)\n",
        "    label = torch.full((args.batch_size,), real_label, device=device)\n",
        "    \n",
        "    for _ in range(args.num_disc_update):\n",
        "        netD.zero_grad()\n",
        "\n",
        "        # train separately with all trues and all fake images\n",
        "        # GANHACK #4 \n",
        "        ## Train with all-real batch\n",
        "        # Forward pass real batch through D\n",
        "\n",
        "        outputR = netD(real_cpu_batch).view(-1)\n",
        "        D_x = outputR.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        # Sampling from Gaussian Space is GANHACKS #3\n",
        "        noise = torch.randn(args.batch_size, args.latent_dim, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "\n",
        "        # Classify all fake batch with D\n",
        "        outputF = netD(fake.detach()).view(-1)\n",
        "        D_G_z1 = outputF.mean().item()\n",
        "\n",
        "        # RalsGAN loss function\n",
        "        ### Relativistic average LSGAN\n",
        "        # https://github.com/AlexiaJM/RelativisticGAN\n",
        "        errD = (torch.mean((outputR - torch.mean(outputF) - label) ** 2) + \n",
        "                torch.mean((outputF - torch.mean(outputR) + label) ** 2))/2\n",
        "        errD.backward(retain_graph=True)\n",
        "\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "        \n",
        "    ############################\n",
        "    # (2) Update G network: maximize log(D(G(z)))\n",
        "    ###########################\n",
        "    netG.zero_grad()\n",
        "    # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "    outputF = netD(fake).view(-1)\n",
        "    # Calculate G's loss based on this output (RalSGAN)\n",
        "    ### Relativistic average LSGAN\n",
        "    # https://github.com/AlexiaJM/RelativisticGAN\n",
        "    errG = (torch.mean((outputR - torch.mean(outputF) + label) ** 2) +\n",
        "            torch.mean((outputF - torch.mean(outputR) - label) ** 2))/2\n",
        "    # Calculate gradients for G\n",
        "    errG.backward()\n",
        "    D_G_z2 = outputF.mean().item()\n",
        "    # Update G\n",
        "    optimizerG.step()\n",
        "    \n",
        "    return {\n",
        "            'errD': errD.item(),\n",
        "            'errG': errG.item(),\n",
        "            'D_x': D_x,\n",
        "            'D_G_z1': D_G_z1,\n",
        "            'D_G_z2': D_G_z2\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_output_dir()\n",
        "\n",
        "# ignite objects\n",
        "trainer = Engine(step)\n",
        "checkpoint_handler = ModelCheckpoint(args.output_dir, args.CKPT_PREFIX, save_interval=1, n_saved=10, require_empty=False)\n",
        "timer = Timer(average=True)\n",
        "\n",
        "# attach running average metrics\n",
        "monitoring_metrics = ['errD', 'errG', 'D_x', 'D_G_z1', 'D_G_z2']\n",
        "RunningAverage(alpha=args.alpha, output_transform=lambda x: x['errD']).attach(trainer, 'errD')\n",
        "RunningAverage(alpha=args.alpha, output_transform=lambda x: x['errG']).attach(trainer, 'errG')\n",
        "RunningAverage(alpha=args.alpha, output_transform=lambda x: x['D_x']).attach(trainer, 'D_x')\n",
        "RunningAverage(alpha=args.alpha, output_transform=lambda x: x['D_G_z1']).attach(trainer, 'D_G_z1')\n",
        "RunningAverage(alpha=args.alpha, output_transform=lambda x: x['D_G_z2']).attach(trainer, 'D_G_z2')\n",
        "\n",
        "# attach progress bar\n",
        "pbar = ProgressBar()\n",
        "pbar.attach(trainer, metric_names=monitoring_metrics)\n",
        "\n",
        "# adding handlers using `trainer.on` decorator API\n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def print_logs(engine):\n",
        "    if (engine.state.iteration - 1) % args.PRINT_FREQ == 0:\n",
        "        fname = os.path.join(args.output_dir, args.LOGS_FNAME)\n",
        "        columns = [\"iteration\", ] + list(engine.state.metrics.keys())\n",
        "        values = [str(engine.state.iteration), ] + \\\n",
        "                 [str(round(value, 5)) for value in engine.state.metrics.values()]\n",
        "        with open(fname, 'a') as f:\n",
        "            if f.tell() == 0:\n",
        "                print('\\t'.join(columns), file=f)\n",
        "            print('\\t'.join(values), file=f)\n",
        "\n",
        "        message = '[{epoch}/{max_epoch}][{i}/{max_i}]'.format(epoch=engine.state.epoch,\n",
        "                                                              max_epoch=args.num_epochs,\n",
        "                                                              i=(engine.state.iteration % len(dataloader)),\n",
        "                                                              max_i=len(dataloader))\n",
        "        for name, value in zip(columns, values):\n",
        "            message += ' | {name}: {value}'.format(name=name, value=value)\n",
        "        pbar.log_message(message)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def save_fake_example(engine):\n",
        "    fake = netG(fixed_noise).detach().cpu()\n",
        "    path = os.path.join(args.output_dir, args.FAKE_IMG_FNAME.format(engine.state.epoch))\n",
        "    vutils.save_image((fake+1.)/2., path, normalize=True)\n",
        "    \n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def save_real_example(engine):\n",
        "    img = engine.state.batch\n",
        "    path = os.path.join(args.output_dir, args.REAL_IMG_FNAME.format(engine.state.epoch))\n",
        "    vutils.save_image(img, path, normalize=True)\n",
        "    \n",
        "# adding handlers using `trainer.add_event_handler` method API\n",
        "trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, \n",
        "                          handler=checkpoint_handler,\n",
        "                          to_save={\n",
        "                              'netG': netG,\n",
        "                              'netD': netD\n",
        "                          })\n",
        "\n",
        "# automatically adding handlers via a special `attach` method of `Timer` handler\n",
        "timer.attach(trainer, \n",
        "             start=Events.EPOCH_STARTED, \n",
        "             resume=Events.EPOCH_STARTED,\n",
        "             pause=Events.EPOCH_COMPLETED, \n",
        "             step=Events.EPOCH_COMPLETED)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def print_times(engine):\n",
        "    pbar.log_message(f'Epoch {engine.state.epoch} done. Time per epoch: {timer.value()/60:.3f}[min]')\n",
        "    timer.reset()\n",
        "    \n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def create_plots(engine):\n",
        "    try:\n",
        "        import matplotlib as mpl\n",
        "        mpl.use('agg')\n",
        "\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "    except ImportError:\n",
        "        warnings.warn('Loss plots will not be generated -- pandas or matplotlib not found')\n",
        "\n",
        "    else:\n",
        "        df = pd.read_csv(os.path.join(args.output_dir, args.LOGS_FNAME), delimiter='\\t', index_col='iteration')\n",
        "        _ = df.loc[:, list(engine.state.metrics.keys())].plot(subplots=True, figsize=(10, 10))\n",
        "        _ = plt.xlabel('Iteration number')\n",
        "        fig = plt.gcf()\n",
        "        path = os.path.join(args.output_dir, args.PLOT_FNAME)\n",
        "        fig.savefig(path)\n",
        "        \n",
        "@trainer.on(Events.EXCEPTION_RAISED)\n",
        "def handle_exception(engine, e):\n",
        "    if isinstance(e, KeyboardInterrupt) and (engine.state.iteration > 1):\n",
        "        engine.terminate()\n",
        "        warnings.warn('KeyboardInterrupt caught. Exiting gracefully.')\n",
        "\n",
        "        create_plots(engine)\n",
        "        checkpoint_handler(engine, {\n",
        "            'netG_exception': netG,\n",
        "            'netD_exception': netD\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        raise e\n",
        "        \n",
        "trainer.run(dataloader, args.num_epochs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/scratch/work/kumary1/dogs/output_images'):\n",
        "    os.mkdir('/scratch/work/kumary1/dogs/output_images')\n",
        "im_batch_size = 50\n",
        "n_images=10000\n",
        "for i_batch in range(0, n_images, im_batch_size):\n",
        "    gen_z = torch.randn(im_batch_size, args.latent_dim, 1, 1, device=device)\n",
        "    gen_images = (netG(gen_z)+1.)/2. # denormalize\n",
        "    images = gen_images.to(\"cpu\").clone().detach()\n",
        "    images = images.numpy().transpose(0, 2, 3, 1)\n",
        "    for i_image in range(gen_images.size(0)):\n",
        "        save_image(gen_images[i_image, :, :, :], os.path.join('/scratch/work/kumary1/dogs/output_images', f'image_{i_batch+i_image:05d}.png'))\n",
        "\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive('images', 'zip', '/scratch/work/kumary1/dogs/output_images')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if COMPUTE_LB:\n",
        "    # UNCOMPRESS OUR IMGAES\n",
        "    shutil.unpack_archive('images.zip', extract_dir='../tmp/images2')\n",
        "\n",
        "    # COMPUTE LB SCORE\n",
        "    m2 = []; s2 =[]; f2 = []\n",
        "    user_images_unzipped_path = '../tmp/images2/'\n",
        "    images_path = [user_images_unzipped_path,'/scratch/work/kumary1/dogs/all-dogs/']\n",
        "    public_path = '/scratch/work/kumary1/dogs/classify_image_graph_def.pb'\n",
        "\n",
        "    fid_epsilon = 10e-15\n",
        "\n",
        "    fid_value_public, distance_public, m2, s2, f2 = calculate_kid_given_paths(images_path, 'Inception', public_path, mm=m2, ss=s2, ff=f2)\n",
        "    distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])\n",
        "    print(\"FID_public: \", fid_value_public, \"distance_public: \", distance_public, \"multiplied_public: \",\n",
        "            fid_value_public /(distance_public + fid_epsilon))\n",
        "    \n",
        "    # REMOVE FILES TO PREVENT KERNEL ERROR OF TOO MANY FILES\n",
        "    ! rm -r ../tmp"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.14.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}